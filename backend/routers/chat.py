from fastapi import APIRouter, HTTPException, Request
import os
import json
import urllib.request
import urllib.error
import re
import difflib
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from services.llm_engine import get_chat_model
from services import memory

# --- INTEGRACIÃ“N RAG ---
try:
    from services.rag_handler import rag_service
except ImportError:
    rag_service = None
    print("âš ï¸ RAG Handler no encontrado.")

router = APIRouter()

def parse_user_message(text: str) -> dict:
    """Extrae datos bÃ¡sicos del texto usando heurÃ­stica."""
    out = {"destination": "", "duration": "", "style": ""}
    if not text: return out
    text_low = text.lower()
    
    # 1. DuraciÃ³n
    m = re.search(r"(\d+)\s*d[iÃ­]a", text_low)
    if m: out["duration"] = m.group(1) + " dÃ­as"
    
    # 2. Estilos
    styles = ["relax", "aventura", "cultural", "gastronÃ³mico", "familia", "lujo", "explorer", "low", "high", "medium"]
    for s in styles:
        if s in text_low:
            out["style"] = s
            break
            
    # 3. Ciudades (Lista bÃ¡sica)
    cities = ["madrid", "barcelona", "sevilla", "valencia", "granada", "bilbao", "malaga", "cordoba", "zaragoza", "santiago", "san sebastian", "ibiza", "mallorca", "tenerife"]
    for c in cities:
        if c in text_low:
            out["destination"] = c.title()
            break
            
    if not out["destination"]:
        m2 = re.search(r"\bviaje a ([A-Za-zÃÃ‰ÃÃ“ÃšÃ‘Ã¡Ã©Ã­Ã³ÃºÃ±]+)\b", text_low)
        if m2: out["destination"] = m2.group(1).title()

    return out

@router.post("/generate")
async def generate_itinerary(request: Request) -> dict:
    try:
        # 1. Leer Payload
        try:
            payload = await request.json()
        except:
            raw = await request.body()
            payload = {"extra_info": raw.decode("utf-8", errors="ignore")}

        # 2. Normalizar campos
        extra_info = (payload.get("extra_info") or payload.get("message") or "").strip()
        dest_in = (payload.get("destination") or "").strip()
        dur_in = (payload.get("duration") or "").strip()
        style_in = (payload.get("style") or "").strip()
        model_in = (payload.get("model") or payload.get("model_name") or None)
        session_id = payload.get("session_id") or "user_1"

        # 3. GestiÃ³n de Memoria
        session = memory.get_session_data(session_id)
        memory_dict = session.setdefault("memory", {"destination": "", "duration": "", "style": ""})
        
        # Actualizar memoria
        if dest_in: memory.update_trip_memory(session_id, dest=dest_in)
        if dur_in: memory.update_trip_memory(session_id, dur=dur_in)
        if style_in: memory.update_trip_memory(session_id, style=style_in)

        # Analizar texto libre para extraer datos
        parsed = parse_user_message(extra_info)
        if parsed["destination"] and not dest_in: memory.update_trip_memory(session_id, dest=parsed["destination"])
        if parsed["duration"] and not dur_in: memory.update_trip_memory(session_id, dur=parsed["duration"])

        # Recuperar estado actual
        dest = (memory_dict.get("destination") or "").strip()
        dur = (memory_dict.get("duration") or "").strip()
        style = (memory_dict.get("style") or "").strip()

        # --- LÃ“GICA DE CONTEXTO RAG INTELIGENTE ---
        rag_context = ""
        
        # A. DETECTAR SI EL MENSAJE CONTIENE ANÃLISIS DE ARCHIVO
        # El frontend puede enviar el anÃ¡lisis directamente en extra_info
        if extra_info and ("[ANÃLISIS" in extra_info or "UBICACIÃ“N:" in extra_info or "TIPO DE ATRACCIÃ“N" in extra_info):
            # El usuario estÃ¡ compartiendo un anÃ¡lisis de archivo
            rag_context = f"\nðŸ“Ž ANÃLISIS DEL ARCHIVO COMPARTIDO:\n{extra_info}\n"
            print(f"âœ… AnÃ¡lisis de archivo detectado: {len(extra_info)} caracteres")
            
            # Simplificar el mensaje para el LLM
            extra_info = "He compartido un archivo/imagen. Por favor, analÃ­zalo segÃºn la informaciÃ³n proporcionada arriba y sugiÃ©reme actividades para el itinerario."
        
        # B. BÃšSQUEDA VECTORIAL (Fallback)
        elif rag_service and extra_info:
            try:
                retrieved = rag_service.retrieve_context(extra_info, session_id, k=3)
                if retrieved and len(retrieved.strip()) > 20:
                    rag_context += f"\nðŸ“Ž INFORMACIÃ“N HISTÃ“RICA:\n{retrieved}"
                    print(f"âœ… Contexto histÃ³rico recuperado")
            except Exception as e:
                print(f"âš ï¸ RAG Error: {e}")

        # 5. GENERAR RESPUESTA (LLM)
        session_history = memory.get_chat_history(session_id)
        response_text = None

        try:
            llm, provider = get_chat_model(model_in)
            print(f"ðŸ›°ï¸ Usando proveedor: {provider}")
            
            # --- PROMPT ANTIBLOQUEO ---
            system_prompt = """Eres RutaÃ‘, experto en viajes por EspaÃ±a con acceso a anÃ¡lisis avanzado de imÃ¡genes mediante visiÃ³n artificial.

REGLAS CRÃTICAS SOBRE IMÃGENES Y ARCHIVOS:
===========================================
âœ… SI VES CONTENIDO CON ETIQUETA "ðŸ“Ž ANÃLISIS DE LA IMAGEN":
   - El usuario HA compartido una imagen/archivo
   - La secciÃ³n "ðŸ“Ž ANÃLISIS" contiene la descripciÃ³n visual de lo que se ve
   - DEBES actuar como si HUBIERAS visto la imagen (porque tienes la descripciÃ³n exacta)
   - Responde: "Veo que...", "SegÃºn la imagen...", "En la foto observo..."
   - NUNCA digas "no puedo ver imÃ¡genes" o "no tengo acceso a visiÃ³n"

âœ… SI NO VES "ðŸ“Ž ANÃLISIS":
   - El usuario NO ha compartido archivo todavÃ­a
   - Solicita mÃ¡s informaciÃ³n sobre destino/fecha/tipo de viaje

LÃ“GICA DE CONVERSACIÃ“N:
=======================
PASO 1 - USUARIO COMPARTE IMAGEN:
   â†’ TÃº: "Veo una foto de [lugar]. Â¿Quieres planificar un viaje allÃ­?"
   â†’ Extrae: ubicaciÃ³n, tipo de atracciÃ³n, actividades

PASO 2 - USUARIO DICE "SÃ, HAZLO":
   â†’ Genera el JSON del itinerario (ver formato abajo)
   â†’ Incluye actividades basadas en la imagen
   
PASO 3 - USUARIO DICE OTRA COSA:
   â†’ ContinÃºa la conversaciÃ³n naturalmente
   â†’ Usa la informaciÃ³n de la imagen como contexto

FORMATO JSON PARA ITINERARIOS (Genera SOLO cuando pida "crear ruta"):
=====================================================================
{{
    "titulo": "Viaje a [Lugar]",
    "dias": [
        {{
            "dia": 1,
            "resumen": "ExploraciÃ³n y primeras impresiones",
            "actividades": [
                {{"activity": "Visita al [Lugar especÃ­fico]", "category": "Sightseeing"}},
                {{"activity": "[Actividad gastronÃ³mica]", "category": "Food"}},
                {{"activity": "[Actividad de relajaciÃ³n]", "category": "Relaxation"}}
            ]
        }}
    ]
}}

CategorÃ­as VÃLIDAS (usa EXACTAMENTE estas):
- Culture: museos, monumentos, galerÃ­as, iglesias
- Food: restaurantes, mercados, gastronomÃ­a
- Hiking: senderismo, montaÃ±a, naturaleza activa
- Relaxation: spa, descanso, playas tranquilas
- Sightseeing: miradores, paseos, tours generales
- General: otra actividad

IMPORTANTE: En Modo Itinerario (JSON), NO aÃ±adas texto fuera del JSON.
En Modo Chat, responde naturalmente como un asistente conversacional."""

            human_input = f"""InformaciÃ³n del sistema:
{rag_context if rag_context else ""}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ESTADO: Destino='{dest}', DuraciÃ³n='{dur}', Estilo='{style}'
USUARIO: {extra_info}

Responde naturalmente. Si ves "ðŸ“Ž ANÃLISIS", confirma que viste la imagen."""

            prompt_template = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
            ])

            if llm:
                chain = prompt_template | llm
                response_obj = chain.invoke({"input": human_input, "chat_history": session_history})
                response_text = response_obj.content if hasattr(response_obj, "content") else str(response_obj)

        except Exception as e:
            print(f"âš ï¸ LLM call failed: {e}")
            response_text = None

        if not response_text:
            return {"es_itinerario": False, "mensaje_chat": "Error tÃ©cnico en el cerebro del asistente."}

        cleaned = response_text.replace("```json", "").replace("```", "").strip()
        m = re.search(r"\{[\s\S]*\}", cleaned)
        
        # Historial (guardamos el mensaje limpio, no el tÃ©cnico enorme)
        memory.add_message_to_history(session_id, "user", extra_info)
        memory.add_message_to_history(session_id, "ai", cleaned)

        if m:
            try:
                return {"es_itinerario": True, **json.loads(m.group(0))}
            except:
                pass
        
        return {"es_itinerario": False, "mensaje_chat": cleaned}

    except Exception as e:
        print(f"âŒ Error in /generate: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/model_check")
async def model_check(request: Request) -> dict:
    # Endpoint que comprueba el proveedor/resolve del modelo solicitado
    try:
        try:
            payload = await request.json()
        except:
            payload = {}

        model = payload.get("model") or payload.get("model_name") or os.getenv("LLM_MODEL", "smart")
        try:
            llm, provider = get_chat_model(model)
            # No devolvemos el objeto llm. Solo el nombre del proveedor/modelo resuelto.
            return {"ok": True, "provider": provider, "model": model}
        except Exception as e:
            return {"ok": False, "message": str(e)}
    except Exception as e:
        return {"ok": False, "message": str(e)}